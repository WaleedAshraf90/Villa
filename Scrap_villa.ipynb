{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0701d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a4d7297153fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tester\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\testing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m from pandas._testing import (\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_testing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3054\u001b[1;33m \u001b[0mcython_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSelectionMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time as t\n",
    "import csv\n",
    "import random\n",
    "import threading as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver= webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c639127",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_lst= []\n",
    "details_lst= []\n",
    "address_lst= []\n",
    "spaces_lst= []\n",
    "bedrooms_lst= []\n",
    "bathrooms_lst= []\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "for i in range(1,21):\n",
    "    driver.get(f'https://aqarmap.com.eg/en/for-sale/villa/?default=20&page={i}')\n",
    "    \n",
    "    # scrape Prices:\n",
    "    try:\n",
    "        Price= driver.find_elements_by_css_selector(\"span[class= 'integer']\")            # job_titles منهم واللي هما بالنسبالي دول ال text وده لان التاج ده فيه منه أكتر من واحد بنفس الكلاس ده انا عندي 15 واحد وبعد كده بلوب عليهم وبطلع ال find_element مش find_elements بكلاس معين فعشان كده بنعمل h2 والوظيفه الواحده التاج بتاعها اسمه job_title أنا عندي في الصفحه الواحده 15 وظيفه يعني 15 \n",
    "        Price_lst1= [element.text for element in Price if len(element.text) >= 6]\n",
    "        price_lst.extend(Price_lst1)\n",
    "    except:\n",
    "         price_lst.append(\"\")\n",
    "\n",
    "    \n",
    "    # scrape Details:\n",
    "    details= driver.find_elements_by_css_selector(\"h2[class= 'search-listing-card__title']\")            # job_titles منهم واللي هما بالنسبالي دول ال text وده لان التاج ده فيه منه أكتر من واحد بنفس الكلاس ده انا عندي 15 واحد وبعد كده بلوب عليهم وبطلع ال find_element مش find_elements بكلاس معين فعشان كده بنعمل h2 والوظيفه الواحده التاج بتاعها اسمه job_title أنا عندي في الصفحه الواحده 15 وظيفه يعني 15 \n",
    "    details_lst1= [element.text for element in details]\n",
    "    details_lst.extend(details_lst1)\n",
    "\n",
    "    \n",
    "# scrape Address:\n",
    "    address= driver.find_elements_by_css_selector(\"p[class= 'search-listing-card__address']\")            # job_titles منهم واللي هما بالنسبالي دول ال text وده لان التاج ده فيه منه أكتر من واحد بنفس الكلاس ده انا عندي 15 واحد وبعد كده بلوب عليهم وبطلع ال find_element مش find_elements بكلاس معين فعشان كده بنعمل h2 والوظيفه الواحده التاج بتاعها اسمه job_title أنا عندي في الصفحه الواحده 15 وظيفه يعني 15 \n",
    "    address_lst1= [element.text for element in address]\n",
    "    address_lst.extend(address_lst1)\n",
    "    \n",
    "    \n",
    "# scrape ground space:\n",
    "    containers= driver.find_elements_by_css_selector(\"div[class= 'search-listing-card__details__wrapper']\")          # job_titles منهم واللي هما بالنسبالي دول ال text وده لان التاج ده فيه منه أكتر من واحد بنفس الكلاس ده انا عندي 15 واحد وبعد كده بلوب عليهم وبطلع ال find_element مش find_elements بكلاس معين فعشان كده بنعمل h2 والوظيفه الواحده التاج بتاعها اسمه job_title أنا عندي في الصفحه الواحده 15 وظيفه يعني 15 \n",
    "    for container in containers:\n",
    "        spaces= container.find_element_by_css_selector(\"div[class= 'search-listing-card__attributes']\").find_element_by_css_selector(\"label:nth-child(1)\").text\n",
    "        spaces_lst.append(spaces)\n",
    "\n",
    "        \n",
    "# scrape bedrooms:\n",
    "    containers= driver.find_elements_by_css_selector(\"div[class= 'search-listing-card__details__wrapper']\")          # job_titles منهم واللي هما بالنسبالي دول ال text وده لان التاج ده فيه منه أكتر من واحد بنفس الكلاس ده انا عندي 15 واحد وبعد كده بلوب عليهم وبطلع ال find_element مش find_elements بكلاس معين فعشان كده بنعمل h2 والوظيفه الواحده التاج بتاعها اسمه job_title أنا عندي في الصفحه الواحده 15 وظيفه يعني 15 \n",
    "    for container in containers:\n",
    "        try:\n",
    "            bedrooms= container.find_element_by_css_selector(\"div[class= 'search-listing-card__attributes']\").find_element_by_css_selector(\"label:nth-child(2)\").text\n",
    "            bedrooms_lst.append(bedrooms)\n",
    "        except:\n",
    "            bedrooms_lst.append(\"\")\n",
    "    \n",
    "    \n",
    "# scrape bathrooms:\n",
    "    containers= driver.find_elements_by_css_selector(\"div[class= 'search-listing-card__details__wrapper']\")          # job_titles منهم واللي هما بالنسبالي دول ال text وده لان التاج ده فيه منه أكتر من واحد بنفس الكلاس ده انا عندي 15 واحد وبعد كده بلوب عليهم وبطلع ال find_element مش find_elements بكلاس معين فعشان كده بنعمل h2 والوظيفه الواحده التاج بتاعها اسمه job_title أنا عندي في الصفحه الواحده 15 وظيفه يعني 15 \n",
    "    for container in containers:\n",
    "        try:\n",
    "            bathrooms= container.find_element_by_css_selector(\"div[class= 'search-listing-card__attributes']\").find_element_by_css_selector(\"label:nth-child(3)\").text\n",
    "            bathrooms_lst.append(bathrooms)\n",
    "        except:\n",
    "            bathrooms_lst.append(\"\")\n",
    "    \n",
    "########################################################\n",
    "    from itertools import zip_longest\n",
    "    from csv import writer\n",
    "\n",
    "    file_list= [price_lst, details_lst, address_lst, spaces_lst, bedrooms_lst, bathrooms_lst]\n",
    "    exported= zip_longest(*file_list)\n",
    "\n",
    "    with open(f'Villlla.csv', 'w', encoding=\"utf-8\") as csv_file:\n",
    "        wr= writer(csv_file)\n",
    "        wr.writerow(['Price', 'Details', 'Address', 'Space', 'Bedroom', 'Bathroom'])\n",
    "        wr.writerows(exported)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Villlla.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb5420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840247b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
